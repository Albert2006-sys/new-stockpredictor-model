Person 2: The Developer
Your Primary Objective: You are the architect of the application services. Your world is APIs, real-time communication, and the user experience. You are responsible for building a robust application that consumes the Quant's models and presents their insights to the user in a clear, live format.

File Ownership:

backend/api/ (all files)

backend/config/ (all files, co-owned with Quant)

backend/ingestion/news_ingestor.py & macro_ingestor.py

backend/ml_models/predict.py

backend/trading_logic/generate_recommendation.py

backend/utils/model_loader.py

backend/Dockerfile

frontend/ (entire directory)

docker-compose.yml

README.md (Application setup & user guide sections)

Day-by-Day Plan for The Developer
Day 1 (Thu, Sep 4): Services & Contextual Data

Key Focus: Set up all project skeletons and the secondary data feeds.

Checklist:

[ ] Set up the FastAPI and Next.js project structures.

[ ] Write backend/ingestion/news_ingestor.py (including sentiment analysis).

[ ] Write backend/ingestion/macro_ingestor.py.

[ ] Write the initial backend/api/main.py to get the server running.

[ ] SYNC: Confirm database access with Person 1.

Day 2 (Fri, Sep 5): The API & Real-Time Channel

Key Focus: Build the communication bridge to the future frontend.

Checklist:

[ ] Write backend/api/endpoints.py to serve historical data.

[ ] Write backend/api/websockets.py. Implement the connection logic and have it send dummy JSON data for now.

[ ] Write backend/trading_logic/generate_recommendation.py to work with dummy model outputs.

[ ] SYNC: Finalize the WebSocket JSON structure with Person 1.

Day 3 & 4 (Sat, Sep 6 - Sun, Sep 7): The Frontend Build

Key Focus: Build the complete Next.js application using the dummy WebSocket data.

Checklist:

[ ] In frontend/lib/api.js, write the function to fetch historical data.

[ ] In frontend/hooks/useWebSocket.js, write the hook to connect to your WebSocket.

[ ] Build all React components (StockChart, PredictionDisplay, etc.).

[ ] Assemble the main UI in frontend/app/page.js. By the end of the weekend, you should have a visually complete, functional dashboard running on mock real-time data.

Day 5 (Mon, Sep 8): Live Prediction Integration

Key Focus: Make the predictions real.

Checklist:

[ ] Write backend/utils/model_loader.py to safely load models from the path in model_config.json.

[ ] Write backend/ml_models/predict.py to use the real .tflite models provided by Person 1.

[ ] Integrate: Update backend/api/websockets.py to stop sending dummy data. It should now call predict.py and generate_recommendation.py to get live results and push them to the frontend.

[ ] SYNC: This is the most critical integration. Work closely with Person 1 to debug any issues.

Day 6 (Tue, Sep 9): Containerization & Orchestration

Key Focus: Make the entire system portable and easy to run.

Checklist:

[ ] Write the Dockerfile for the backend.

[ ] Write the Dockerfile for the frontend.

[ ] Write the docker-compose.yml file, defining and linking the database, all ingestion scripts, the API, and the frontend. Test that docker-compose up successfully starts everything.

Day 7 (Wed, Sep 10): Testing & Documentation

Key Focus: Find and fix bugs. Prepare for submission.

Checklist:

[ ] Lead the end-to-end testing of the full application running via Docker Compose.

[ ] Report bugs and work with Person 1 to resolve them.

[ ] Write the user-facing setup and usage instructions in the README.md.





1. backend/api/ (all files)
Core Objective: To create the central nervous system of the application. This API will serve historical data, manage the live WebSocket connections, and be the sole entry point for the frontend.

File Breakdown:

main.py:

Objective: To initialize and configure the FastAPI application.

Components:

Import FastAPI.

Create the main app instance: app = FastAPI().

Implement CORS middleware to allow the Next.js frontend (running on a different port) to communicate with it.

Import and include the routers from endpoints.py and websockets.py.

Define a root endpoint (@app.get("/")) for a basic health check.

endpoints.py:

Objective: To provide standard RESTful endpoints for the frontend to fetch data it needs on initial page load.

Components:

Create an APIRouter.

Define the endpoint GET /stocks/{ticker}/historical: This function will take a stock ticker as input. It will use the database_manager to query TimescaleDB for the last X days of 1-minute price data. It must handle cases where the ticker is invalid.

Define the endpoint GET /stocks/list: A simple endpoint that returns the static list of the 20 stocks the system is tracking.

websockets.py:

Objective: This is the heart of the real-time functionality. It manages persistent connections with clients and pushes live data.

Components:

Create an APIRouter.

Define a WebSocket endpoint: @router.websocket("/ws/live_updates").

Connection Manager: Implement a simple class to manage active WebSocket connections (add new connections, disconnect, broadcast messages).

The "Push" Loop: This is the most critical part. It needs a separate, continuously running task (using asyncio.create_task or a similar library like apscheduler) that does the following every few seconds:
a.  Fetches the absolute latest 60 minutes of feature data for all 20 stocks from the database.
b.  For each stock, it calls the predict.py function for both the LSTM and Transformer models.
c.  It then calls the generate_recommendation.py function, passing it the two model outputs.
d.  It bundles the latest price, the two predictions, and the final recommendation into a single JSON object.
e.  It broadcasts this JSON object to all connected WebSocket clients.

Inputs: Requests from the frontend; data from the database; model predictions.

Outputs: JSON data over REST; JSON data over WebSockets.

Critical Precautions:

Asynchronous Code: FastAPI and WebSockets are asynchronous. All database calls and long-running operations within the API must use async and await to avoid blocking the server.

Efficiency: The push loop in the WebSocket needs to be highly efficient. It should perform its data fetching and prediction tasks as quickly as possible to maintain the "live" feel.

2. backend/ingestion/news_ingestor.py & macro_ingestor.py
Core Objective: To provide the contextual, non-price data that enriches the models' predictions. These are standalone, resilient services.

news_ingestor.py:

Components:

A main loop that runs continuously.

Inside the loop, it queries a news API for recent articles related to the 20 stocks.

It needs a mechanism to avoid processing the same article twice (e.g., by storing article URLs in the database).

For each new article, it loads a pre-trained sentiment analysis model (e.g., from Hugging Face) and calculates a sentiment score.

It writes the ticker, timestamp, and sentiment score to a news_sentiment table in the database.

Includes time.sleep() to respect news API rate limits.

macro_ingestor.py:

Components:

A script designed to be run once per day by a scheduler (cron).

It connects to the FRED API.

It fetches the latest values for a predefined list of economic indicators.

It writes this data to a macro_data table.

Inputs: News APIs, FRED API.

Outputs: Data written to TimescaleDB.

Critical Precautions:

Decoupling: These services must be completely independent of the main API. If the news ingestor fails, it should not affect the price ingestor or the API.

Resource Management: Loading the sentiment model can be memory-intensive. This script must be designed to load the model once on startup, not in every loop iteration.

3. backend/ml_models/predict.py
Core Objective: To provide a clean, simple, and fast interface for getting a prediction from a single, compiled .tflite model.

Key Components:

make_prediction(model_interpreter, data_sequence): A single, primary function.

Logic:

Takes a loaded TensorFlow Lite interpreter object and a prepared data sequence (e.g., a NumPy array of shape (1, 60, num_features)).

Sets the input tensor of the interpreter.

Invokes the interpreter (interpreter.invoke()).

Gets the output tensor. This will be an array of three probabilities (e.g., [0.2, 0.1, 0.7]).

Interprets the output: finds the index of the highest probability and maps it to the corresponding label (-1, 0, or 1).

Returns both the final label and the full probability array.

Inputs: A TFLite interpreter object, a NumPy array of feature data.

Outputs: A prediction label and a probability array.

Critical Precautions:

Stateless: This script should be completely stateless. It doesn't load models or data itself; it only performs the calculation. This makes it highly reusable and easy to test.

Speed: The logic here must be minimal. No heavy processing, just the raw inference call.

4. backend/trading_logic/generate_recommendation.py
Core Objective: To act as the final decision-making layer, translating raw model probabilities into actionable, human-readable trading advice.

Key Components:

get_trade_recommendation(lstm_output, transformer_output): A single, primary function.

Logic:

Takes the output (label and probabilities) from both the LSTM and Transformer models.

Implements the "dual-model confirmation" logic. For example:

if lstm_label == 1 and transformer_label == 1 and lstm_prob > 0.7 and transformer_prob > 0.7: -> High-Confidence "Buy"

if lstm_label == -1 and transformer_label == -1 and lstm_prob > 0.7 and transformer_prob > 0.7: -> High-Confidence "Sell"

else: -> "Hold"

Constructs a recommendation object that includes the final decision, the confidence level ("High", "Low"), and a clear disclaimer.

Inputs: The prediction objects from both models.

Outputs: A recommendation object (e.g., a Python dictionary).

Critical Precautions:

Disclaimer: Every single output from this file must include a disclaimer that this is not financial advice. This is a legal and ethical necessity.

Rule Clarity: The rules for combining the model outputs should be clear, simple, and easy to understand. Avoid overly complex logic that makes the final decision opaque.

5. backend/utils/model_loader.py
Core Objective: To safely load the active .tflite models into memory based on the model_config.json file.

Key Components:

load_active_models(): A primary function.

Logic:

Reads the config/model_config.json file.

Gets the file paths for the current "champion" LSTM and Transformer models.

Uses tflite.Interpreter() to load each model file into an interpreter object.

Allocates tensors for each interpreter.

Returns a dictionary containing both loaded interpreter objects (e.g., {'lstm': lstm_interpreter, 'transformer': transformer_interpreter}).

Inputs: model_config.json.

Outputs: A dictionary of loaded TFLite interpreter objects.

Critical Precautions:

Error Handling: This script must handle cases where the model file is missing or corrupt. It should log a critical error if a model cannot be loaded.

6. frontend/ (Entire Directory)
Core Objective: To create a fast, responsive, and intuitive user interface that displays the live data and predictions from the backend.

Key Components:

lib/api.js: Contains typed functions for fetching data from the backend REST endpoints.

hooks/useWebSocket.js: A custom React hook that is the heart of the frontend's real-time capability. It will:

Establish and maintain the WebSocket connection.

Listen for incoming messages.

Parse the JSON data.

Use a state management approach (like useState or Zustand) to provide the latest data to the rest of the application.

Handle reconnection logic if the connection drops.

components/StockChart.js: A component that takes historical and live data as props. It uses a charting library (like ECharts or Lightweight Charts) to render the price data and should have a method to append new live ticks without redrawing the entire chart.

components/PredictionDisplay.js: A component that takes the latest recommendation object as a prop and displays the "Buy"/"Sell"/"Hold" signal with clear visual styling (e.g., green for buy, red for sell).

app/page.js: The main page that assembles all the components. It will use the useWebSocket hook to get the live data and pass it down as props to the chart and display components. It will also use the api.js library to fetch the initial historical data on page load.

Inputs: User interaction; data from the backend API and WebSocket.

Outputs: A rendered web page.

Critical Precautions:

Performance: The frontend must be efficient. Avoid re-rendering the entire page on every new tick. Only the components that need to change should update.

State Management: The flow of data from the WebSocket hook to the components must be clean and predictable.

7. docker-compose.yml
Core Objective: To be the master orchestrator for the entire application, allowing it to be started and stopped with a single command.

Key Components:

services:: Defines each independent part of your application.

db: The TimescaleDB service, with a persistent volume for data.

price_ingestor: The service running the price_ingestor.py script.

news_ingestor: The service running the news_ingestor.py script.

api: The FastAPI backend service.

frontend: The Next.js frontend service.

networks:: Defines a shared network so all the containers can communicate with each other using their service names (e.g., the api service can connect to the db service at host: db).

Critical Precautions:

Dependencies: Use depends_on to ensure services start in the correct order (e.g., the api and ingestors should wait for the db to be healthy).

Environment Variables: Use environment variables within the Compose file to pass configuration (like database credentials) to the services, keeping them out of the code.